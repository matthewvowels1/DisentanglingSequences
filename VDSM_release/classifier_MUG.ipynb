{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "from natsort import natsorted\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim\n",
    "# this script is to be used after \n",
    "# see https://github.com/yatindandi/Disentangled-Sequential-Autoencoder/blob/master/classifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set filenames and parameters etc.\n",
    "root = './codes/release_test/'\n",
    "nc = 3\n",
    "seq_len = 20\n",
    "imsize = 64\n",
    "test_frac = 0.3\n",
    "files = natsorted(os.listdir(root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id (200, 10, 1, 15) 0.022323437\n",
      "(2000, 15) 0.022323437\n",
      "act (200, 10, 50) -0.0013404469\n",
      "(2000, 50) -0.0014967673\n",
      "recon (200, 10, 20, 3, 64, 64) 1.2447744e-05\n",
      "(2000, 20, 3, 64, 64) 1.2447744e-05\n"
     ]
    }
   ],
   "source": [
    "# import data and create train test splits etc.\n",
    "id_codes_files = []\n",
    "action_codes_files = []\n",
    "labels_files = []\n",
    "recon_files = []\n",
    "test_img_files = []\n",
    "sampled_img_files = []\n",
    "for file in files:\n",
    "    if 'dynamics' in file and 'label' not in file:\n",
    "        action_codes_files.append(file)\n",
    "    elif 'id' in file and 'label' not in file:\n",
    "        id_codes_files.append(file)\n",
    "    elif 'labels' in file:\n",
    "        labels_files.append(file)\n",
    "    elif 'recon' in file:\n",
    "        recon_files.append(file)\n",
    "    elif 'test_images' in file:\n",
    "        test_img_files.append(file)\n",
    "    elif 'sampled_images' in file:\n",
    "        sampled_img_files.append(file)\n",
    "         \n",
    "id_codes = []\n",
    "action_codes = []\n",
    "labels = []\n",
    "recon_gen = []\n",
    "test_imgs = []\n",
    "sampled_imgs = []\n",
    "\n",
    "for file in id_codes_files:\n",
    "    id_codes.append(np.load(os.path.join(root, model, file))['arr_0'])\n",
    "for file in action_codes_files:\n",
    "    action_codes.append(np.load(os.path.join(root, model, file))['arr_0'])\n",
    "np_load_old = np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "for file in labels_files:\n",
    "    labels.append(np.load(os.path.join(root, model, file))['arr_0'])\n",
    "for file in test_img_files:\n",
    "    test_imgs.append(np.load(os.path.join(root,model,file))['arr_0'])\n",
    "for file in recon_files:\n",
    "    recon_gen.append(np.load(os.path.join(root,model,file))['arr_0'])\n",
    "for file in sampled_img_files:\n",
    "    sampled_imgs.append(np.load(os.path.join(root,model,file))['arr_0'])\n",
    "np.load = np_load_old\n",
    "    \n",
    "id_codes = np.asarray(id_codes)\n",
    "print('id', id_codes.shape, id_codes[1,0,0,0])\n",
    "id_codes = id_codes.reshape(-1, id_codes.shape[-1])\n",
    "print(id_codes.shape, id_codes[10,0])\n",
    "action_codes = np.asarray(action_codes)\n",
    "print('act', action_codes.shape, action_codes[1,0,0])\n",
    "action_codes = action_codes.reshape(-1, action_codes.shape[-1])\n",
    "print(action_codes.shape, action_codes[6,0])\n",
    "recon_gen = np.asarray(recon_gen)\n",
    "print('recon', recon_gen.shape, recon_gen[1,0,0,0,0,0])\n",
    "recon_gen = recon_gen.reshape(recon_gen.shape[0]*recon_gen.shape[1], seq_len, nc, imsize, imsize)        \n",
    "print(recon_gen.shape, recon_gen[10, 0,0,0,0])\n",
    "test_imgs = np.asarray(test_imgs)\n",
    "test_imgs = test_imgs.reshape(test_imgs.shape[0]*test_imgs.shape[1], seq_len, nc, imsize, imsize)\n",
    "sampled_imgs = np.asarray(sampled_imgs)\n",
    "sampled_imgs = sampled_imgs.reshape(sampled_imgs.shape[0]*sampled_imgs.shape[1], seq_len, nc, imsize, imsize)\n",
    "\n",
    "ids = []\n",
    "actions = []\n",
    "for i in range(len(labels)):\n",
    "    num_in_batch = len(labels[0])\n",
    "    \n",
    "    for j in range(num_in_batch):\n",
    "        ids.append(labels[i][j][0])\n",
    "        actions.append(labels[i][j][1])\n",
    "        \n",
    "        \n",
    "ids = np.asarray(ids).reshape(-1, 1)\n",
    "actions = np.asarray(actions).reshape(-1, 1)\n",
    "labels = np.concatenate((ids, actions), 1)\n",
    "\n",
    "index = np.random.randint(len(labels), size=len(labels))\n",
    "test_indices = index[:int(len(labels)*test_frac)]\n",
    "train_indices = index[int(len(labels)*test_frac):]\n",
    "\n",
    "real_imgs_tr = test_imgs[train_indices]\n",
    "real_imgs_te = test_imgs[test_indices]\n",
    "gen_imgs_tr = recon_gen[train_indices]\n",
    "gen_imgs_te = recon_gen[test_indices]\n",
    "labels_tr = labels[train_indices]\n",
    "labels_te = labels[test_indices]\n",
    "\n",
    "action_codes_tr = action_codes[train_indices]\n",
    "action_codes_te = action_codes[test_indices]\n",
    "id_codes_tr = id_codes[train_indices]\n",
    "id_codes_te = id_codes[test_indices]\n",
    "\n",
    "\n",
    "\n",
    "print(real_imgs_tr.shape, real_imgs_te.shape)\n",
    "print(action_codes_tr.shape, action_codes_te.shape)\n",
    "print(id_codes_tr.shape, id_codes_te.shape)\n",
    "real_fake_train = np.concatenate((real_imgs_tr, gen_imgs_tr),0)\n",
    "real_fake_test = np.concatenate((real_imgs_te, gen_imgs_te),0)\n",
    "labels_both_train = np.concatenate((labels_tr, labels_tr), 0)\n",
    "labels_both_te = np.concatenate((labels_te, labels_te), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 20, 3, 64, 64) (600, 20, 3, 64, 64)\n",
      "(1400, 50) (600, 50)\n",
      "(1400, 15) (600, 15)\n"
     ]
    }
   ],
   "source": [
    "# Dataset objects\n",
    "\n",
    "class MUG(data.Dataset):\n",
    "    def __init__(self, imgs, labels):\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imgs = self.imgs[idx]\n",
    "        labels = self.labels[idx]\n",
    "        return imgs, labels\n",
    "\n",
    "class Codes(data.Dataset):\n",
    "    def __init__(self, codes, labels):\n",
    "        self.codes = codes\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.codes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        codes = self.codes[idx]\n",
    "        labels = self.labels[idx]\n",
    "        return codes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define classifier model and metrics\n",
    "class MUGClassifier(nn.Module):\n",
    "    def __init__(self, n_ids=52, n_actions=9,\n",
    "                 num_frames=seq_len, in_size=64, channels=64, code_dim=1024, hidden_dim=512, nonlinearity=None):\n",
    "        super(MUGClassifier, self).__init__()\n",
    "        nl = nn.LeakyReLU(0.2) if nonlinearity is None else nonlinearity\n",
    "        encoding_conv = []\n",
    "        encoding_conv.append(nn.Sequential(nn.Conv2d(3, channels, 5, 4, 1, bias=False), nl))\n",
    "        size = in_size // 4\n",
    "        self.num_frames = num_frames\n",
    "        while size > 4:\n",
    "            encoding_conv.append(nn.Sequential(\n",
    "                nn.Conv2d(channels, channels * 2, 5, 4, 1, bias=False),\n",
    "                nn.BatchNorm2d(channels * 2), nl))\n",
    "            size = size // 4\n",
    "            channels *= 2\n",
    "        self.encoding_conv = nn.Sequential(*encoding_conv)\n",
    "        self.final_size = size\n",
    "        self.final_channels = channels\n",
    "        self.code_dim = code_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.encoding_fc = nn.Sequential(\n",
    "                nn.Linear(size * size * channels, code_dim),\n",
    "                nn.BatchNorm1d(code_dim), nl)\n",
    "        # The last hidden state of a convolutional LSTM over the scenes is used for classification\n",
    "        self.classifier_lstm = nn.LSTM(code_dim, hidden_dim, batch_first=True, bidirectional=False)\n",
    "        self.id = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "                nn.BatchNorm1d(hidden_dim // 2), nl,\n",
    "                nn.Linear(hidden_dim // 2, n_ids))\n",
    "        self.action = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "                nn.BatchNorm1d(hidden_dim // 2), nl,\n",
    "                nn.Linear(hidden_dim // 2, n_actions))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, x.size(2), x.size(3), x.size(4))\n",
    "        x = self.encoding_conv(x)\n",
    "        x = x.view(-1, self.final_channels * (self.final_size ** 2))\n",
    "        x = self.encoding_fc(x)\n",
    "        x = x.view(-1, self.num_frames, self.code_dim)\n",
    "        # Classifier output depends on last layer of LSTM: Can also change this to a bi-LSTM if required\n",
    "        _, (hidden, _) = self.classifier_lstm(x)\n",
    "        hidden = hidden.view(-1, self.hidden_dim)\n",
    "        return self.id(hidden), self.action(hidden)\n",
    "\n",
    "def check_accuracy(model, test_data, device):\n",
    "    total = 0\n",
    "    correct_id = 0\n",
    "    correct_action = 0\n",
    "    with torch.no_grad():\n",
    "        for item in test_data:\n",
    "            image, label = item\n",
    "            image = image.to(device)\n",
    "            id_ = label[:, 0].to(device)\n",
    "            action = label[:, 1].to(device)\n",
    "            pred_id, pred_action = model(image)\n",
    "            _, pred_id = torch.max(pred_id.data, 1)\n",
    "            _, pred_action = torch.max(pred_action.data, 1)\n",
    "            total += id_.size(0)\n",
    "            correct_id += (pred_id == id_).sum().item()\n",
    "            correct_action += (pred_action == action).sum().item()\n",
    "    print('Accuracy, id : {}  Action {}'.format(correct_id/total, correct_action/total)) \n",
    "\n",
    "\n",
    "def train_classifier(model, optim, train_data, device, epochs, path, test_data, start=0):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(start, epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, item in enumerate(train_data, 1):\n",
    "            image, label = item\n",
    "            image = image.to(device)\n",
    "            id_ = label[:, 0].to(device)\n",
    "            action = label[:, 1].to(device)\n",
    "            pred_id, pred_action = model(image)\n",
    "            loss = criterion(pred_id, id_) + criterion(pred_action, action)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            running_loss += loss.item()\n",
    "        print('Epoch {} Avg Loss {}'.format(epoch + 1, running_loss / i))\n",
    "#         save_model(model, optim, epoch, path)\n",
    "        check_accuracy(model, test_data, device)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Avg Loss 2.3598535643382506\n",
      "Accuracy, id : 0.895  Action 0.6233333333333333\n",
      "Epoch 2 Avg Loss 1.1181245690042323\n",
      "Accuracy, id : 0.9566666666666667  Action 0.765\n",
      "Epoch 3 Avg Loss 0.7033617293292825\n",
      "Accuracy, id : 0.98  Action 0.7966666666666666\n",
      "Epoch 4 Avg Loss 0.5556173095987602\n",
      "Accuracy, id : 0.9633333333333334  Action 0.8233333333333334\n",
      "Epoch 5 Avg Loss 0.48750003977594053\n",
      "Accuracy, id : 0.9716666666666667  Action 0.8516666666666667\n",
      "Epoch 6 Avg Loss 0.4349214125593955\n",
      "Accuracy, id : 0.995  Action 0.88\n",
      "Epoch 7 Avg Loss 0.38797594217414205\n",
      "Accuracy, id : 0.985  Action 0.8766666666666667\n",
      "Epoch 8 Avg Loss 0.38943881168961525\n",
      "Accuracy, id : 0.99  Action 0.9016666666666666\n",
      "Epoch 9 Avg Loss 0.3449688679327003\n",
      "Accuracy, id : 0.9933333333333333  Action 0.8816666666666667\n",
      "Epoch 10 Avg Loss 0.3073172631927512\n",
      "Accuracy, id : 0.9916666666666667  Action 0.8983333333333333\n",
      "Epoch 11 Avg Loss 0.2966213844377886\n",
      "Accuracy, id : 0.99  Action 0.9416666666666667\n",
      "Epoch 12 Avg Loss 0.3104266249468889\n",
      "Accuracy, id : 0.9966666666666667  Action 0.9216666666666666\n",
      "Epoch 13 Avg Loss 0.27456380862911994\n",
      "Accuracy, id : 0.9933333333333333  Action 0.92\n",
      "Epoch 14 Avg Loss 0.26980400339446287\n",
      "Accuracy, id : 0.995  Action 0.915\n",
      "Epoch 15 Avg Loss 0.23662281349640002\n",
      "Accuracy, id : 0.9983333333333333  Action 0.9666666666666667\n",
      "Epoch 16 Avg Loss 0.2386226705169644\n",
      "Accuracy, id : 0.995  Action 0.9483333333333334\n",
      "Epoch 17 Avg Loss 0.2945316407342695\n",
      "Accuracy, id : 0.9916666666666667  Action 0.9566666666666667\n",
      "Epoch 18 Avg Loss 0.2456891632808203\n",
      "Accuracy, id : 0.9966666666666667  Action 0.925\n",
      "Epoch 19 Avg Loss 0.3036833344535394\n",
      "Accuracy, id : 0.9883333333333333  Action 0.9183333333333333\n",
      "Epoch 20 Avg Loss 0.29230014726900583\n",
      "Accuracy, id : 0.995  Action 0.955\n",
      "Epoch 21 Avg Loss 0.36364299339898437\n",
      "Accuracy, id : 0.9816666666666667  Action 0.9033333333333333\n",
      "Epoch 22 Avg Loss 0.45350788959132676\n",
      "Accuracy, id : 0.9933333333333333  Action 0.94\n",
      "Epoch 23 Avg Loss 0.38558247564783826\n",
      "Accuracy, id : 0.97  Action 0.9383333333333334\n",
      "Epoch 24 Avg Loss 0.35491598750972614\n",
      "Accuracy, id : 0.99  Action 0.93\n",
      "Epoch 25 Avg Loss 0.47237614673477685\n",
      "Accuracy, id : 0.9683333333333334  Action 0.8766666666666667\n",
      "Epoch 26 Avg Loss 0.5068392271853306\n",
      "Accuracy, id : 0.9883333333333333  Action 0.9366666666666666\n",
      "Epoch 27 Avg Loss 0.4040618164210834\n",
      "Accuracy, id : 0.9966666666666667  Action 0.92\n",
      "Epoch 28 Avg Loss 0.4601171299899844\n",
      "Accuracy, id : 0.985  Action 0.8883333333333333\n",
      "Epoch 29 Avg Loss 0.4865527182648128\n",
      "Accuracy, id : 0.995  Action 0.905\n",
      "Epoch 30 Avg Loss 0.5985281533476982\n",
      "Accuracy, id : 0.9933333333333333  Action 0.8816666666666667\n",
      "Epoch 31 Avg Loss 0.6588978753848509\n",
      "Accuracy, id : 0.9833333333333333  Action 0.835\n",
      "Epoch 32 Avg Loss 0.69895683220503\n",
      "Accuracy, id : 0.9833333333333333  Action 0.855\n",
      "Epoch 33 Avg Loss 0.762355634349991\n",
      "Accuracy, id : 0.98  Action 0.8833333333333333\n",
      "Epoch 34 Avg Loss 1.0224603355269541\n",
      "Accuracy, id : 0.95  Action 0.795\n",
      "Epoch 35 Avg Loss 1.3730032584545286\n",
      "Accuracy, id : 0.9683333333333334  Action 0.8083333333333333\n",
      "Epoch 36 Avg Loss 1.2058370287784121\n",
      "Accuracy, id : 0.9583333333333334  Action 0.8183333333333334\n",
      "Epoch 37 Avg Loss 1.1264788352630355\n",
      "Accuracy, id : 0.975  Action 0.8016666666666666\n",
      "Epoch 38 Avg Loss 1.3520385548472404\n",
      "Accuracy, id : 0.9683333333333334  Action 0.8133333333333334\n",
      "Epoch 39 Avg Loss 1.2654288292608478\n",
      "Accuracy, id : 0.9733333333333334  Action 0.7966666666666666\n",
      "Epoch 40 Avg Loss 1.003764833238992\n",
      "Accuracy, id : 0.975  Action 0.79\n",
      "Epoch 41 Avg Loss 0.9820116489109668\n",
      "Accuracy, id : 0.955  Action 0.8316666666666667\n",
      "Epoch 42 Avg Loss 0.9874371039596471\n",
      "Accuracy, id : 0.9766666666666667  Action 0.8116666666666666\n",
      "Epoch 43 Avg Loss 1.110276918519627\n",
      "Accuracy, id : 0.9733333333333334  Action 0.7866666666666666\n",
      "Epoch 44 Avg Loss 1.0485420840877024\n",
      "Accuracy, id : 0.975  Action 0.8033333333333333\n",
      "Epoch 45 Avg Loss 1.1556715232066133\n",
      "Accuracy, id : 0.9733333333333334  Action 0.8\n",
      "Epoch 46 Avg Loss 0.8678246694193645\n",
      "Accuracy, id : 0.99  Action 0.8116666666666666\n",
      "Epoch 47 Avg Loss 0.9599579249254682\n",
      "Accuracy, id : 0.98  Action 0.8016666666666666\n",
      "Epoch 48 Avg Loss 0.9975445075807247\n",
      "Accuracy, id : 0.9766666666666667  Action 0.795\n",
      "Epoch 49 Avg Loss 0.9832566263662144\n",
      "Accuracy, id : 0.9866666666666667  Action 0.8033333333333333\n",
      "Epoch 50 Avg Loss 0.9165954596617005\n",
      "Accuracy, id : 0.9616666666666667  Action 0.8083333333333333\n"
     ]
    }
   ],
   "source": [
    "# init model and train\n",
    "device = torch.device('cuda:0')\n",
    "model = MUGClassifier()\n",
    "model.to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "\n",
    "mug_train = MUG(real_fake_train, labels_both_train)\n",
    "# sprites_test = Sprites(real_fake_test, labels_both_te)\n",
    "# sprites_train = Sprites(real_imgs_tr, labels_tr)\n",
    "mug_test = MUG(gen_imgs_te, labels_te)\n",
    "loader = data.DataLoader(mug_train, batch_size=32, shuffle=True, num_workers=4)\n",
    "loader_test = data.DataLoader(mug_test, batch_size=64, shuffle=True, num_workers=4)\n",
    "train_classifier(model, optim, loader, device, 50, './checkpoint_classifier.pth', loader_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# compare real image preds against fake image preds\n",
    "bs = 2\n",
    "real_test = MUG(real_imgs_te, labels_te)\n",
    "fake_test = MUG(gen_imgs_te, labels_te)\n",
    "real_loader = data.DataLoader(real_test, batch_size=bs, shuffle=False, num_workers=4)\n",
    "fake_loader = data.DataLoader(fake_test, batch_size=bs, shuffle=False, num_workers=4)\n",
    "\n",
    "total = 0\n",
    "correct_id = 0\n",
    "correct_action = 0\n",
    "model.eval()\n",
    "for i, (item_real, item_fake) in enumerate(zip(real_loader, fake_loader)):\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "    fake_image, _ = item_fake\n",
    "    fake_image = fake_image.to(device)\n",
    "    image, _ = item_real\n",
    "    image = image.to(device)\n",
    "    im_check = image[0,0].permute(1,2,0).detach().cpu().numpy()\n",
    "    im_check_fake = fake_image[0,0].permute(1,2,0).detach().cpu().numpy()\n",
    "\n",
    "    # real\n",
    "    pred_id, pred_action = model(image)\n",
    "    _, pred_id = torch.max(pred_id.data, 1)\n",
    "    _, pred_action = torch.max(pred_action.data, 1)\n",
    "    # fake\n",
    "    fake_pred_id, fake_pred_action = model(fake_image)\n",
    "    _, fake_pred_id = torch.max(fake_pred_id.data, 1)\n",
    "    _, fake_pred_action = torch.max(fake_pred_action.data, 1)\n",
    "    total += pred_id.size(0)\n",
    "    correct_id += (pred_id == fake_pred_id).sum().item()\n",
    "    correct_action += (pred_action == fake_pred_action).sum().item()\n",
    "print('Accuracy, id : {} Action {}'.format(correct_id/total, correct_action/total)) \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n",
      "600\n",
      "610\n",
      "620\n",
      "630\n",
      "640\n",
      "650\n",
      "660\n",
      "670\n",
      "680\n",
      "690\n",
      "700\n",
      "710\n",
      "720\n",
      "730\n",
      "740\n",
      "750\n",
      "760\n",
      "770\n",
      "780\n",
      "790\n",
      "800\n",
      "810\n",
      "820\n",
      "830\n",
      "840\n",
      "850\n",
      "860\n",
      "870\n",
      "880\n",
      "890\n",
      "900\n",
      "910\n",
      "920\n",
      "930\n",
      "940\n",
      "950\n",
      "960\n",
      "970\n",
      "980\n",
      "990\n",
      "1000\n",
      "1010\n",
      "1020\n",
      "1030\n",
      "1040\n",
      "1050\n",
      "1060\n",
      "1070\n",
      "1080\n",
      "1090\n",
      "1100\n",
      "1110\n",
      "1120\n",
      "1130\n",
      "1140\n",
      "1150\n",
      "1160\n",
      "1170\n",
      "1180\n",
      "1190\n",
      "1200\n",
      "1210\n",
      "1220\n",
      "1230\n",
      "1240\n",
      "1250\n",
      "1260\n",
      "1270\n",
      "1280\n",
      "1290\n",
      "1300\n",
      "1310\n",
      "1320\n",
      "1330\n",
      "1340\n",
      "1350\n",
      "1360\n",
      "1370\n",
      "1380\n",
      "1390\n",
      "1400\n",
      "1410\n",
      "1420\n",
      "1430\n",
      "1440\n",
      "1450\n",
      "1460\n",
      "1470\n",
      "1480\n",
      "1490\n",
      "1500\n",
      "1510\n",
      "1520\n",
      "1530\n",
      "1540\n",
      "1550\n",
      "1560\n",
      "1570\n",
      "1580\n",
      "1590\n",
      "1600\n",
      "1610\n",
      "1620\n",
      "1630\n",
      "1640\n",
      "1650\n",
      "1660\n",
      "1670\n",
      "1680\n",
      "1690\n",
      "1700\n",
      "1710\n",
      "1720\n",
      "1730\n",
      "1740\n",
      "1750\n",
      "1760\n",
      "1770\n",
      "1780\n",
      "1790\n",
      "1800\n",
      "1810\n",
      "1820\n",
      "1830\n",
      "1840\n",
      "1850\n",
      "1860\n",
      "1870\n",
      "1880\n",
      "1890\n",
      "1900\n",
      "1910\n",
      "1920\n",
      "1930\n",
      "1940\n",
      "1950\n",
      "1960\n",
      "1970\n",
      "1980\n",
      "1990\n"
     ]
    }
   ],
   "source": [
    "# INTER/INTRA ENTROPY AND IS KL\n",
    "\n",
    "dummy_labels = torch.zeros(len(sampled_imgs), 2)\n",
    "# compare real image preds against fake image preds\n",
    "bs = 1\n",
    "sampled_test = MUG(sampled_imgs, dummy_labels)  # here labels_te is a dummy \n",
    "sampled_loader = data.DataLoader(sampled_test, batch_size=bs, shuffle=False, num_workers=4)\n",
    "\n",
    "pred_ids = []\n",
    "pred_actions = []\n",
    "model.eval()\n",
    "sm = nn.Softmax(dim=1)\n",
    "for i, item_sampled in enumerate(sampled_loader):\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "    sampled_image, _ = item_sampled\n",
    "    sampled_image = sampled_image.to(device)\n",
    "    # real\n",
    "    pred_id, pred_action = model(sampled_image)\n",
    "    pred_ids.append(sm(pred_id).detach().cpu().numpy())\n",
    "    pred_actions.append(sm(pred_action).detach().cpu().numpy())\n",
    "\n",
    "pred_actions = np.asarray(pred_actions)[:,0]\n",
    "pred_ids = np.asarray(pred_ids)[:,0]\n",
    "eps = 1e-16\n",
    "\n",
    "pred_actions_y = pred_actions.mean(axis=0)\n",
    "inter_entropy_actions = -(pred_actions_y * np.log(pred_actions_y + eps)).sum(0)\n",
    "\n",
    "pred_ids_y = pred_ids.mean(axis=0)\n",
    "inter_entropy_ids = -(pred_ids_y * np.log(pred_ids_y + eps)).sum(0)\n",
    "\n",
    "intra_entropy_actions = -(pred_actions * np.log(pred_actions + eps)).sum(1).mean()\n",
    "intra_entropy_ids = -(pred_ids * np.log(pred_ids + eps)).sum(1).mean()\n",
    "\n",
    "kl_ids = (pred_ids * (np.log(pred_ids + eps) - np.log(pred_ids_y + eps))).sum(1).mean()\n",
    "kl_actions = (pred_actions * (np.log(pred_actions + eps) - np.log(pred_actions_y + eps))).sum(1).mean()\n",
    "\n",
    "average_inter_entropy = (inter_entropy_ids + inter_entropy_actions)/2\n",
    "average_intra_entropy = (intra_entropy_actions + intra_entropy_ids)/2\n",
    "average_kl = (kl_ids + kl_actions)/2\n",
    "print('av. inter_ent', average_inter_entropy, 'av intra ent', average_intra_entropy, 'av kl', average_kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test disentanglement by comparing prediction using id and action codes\n",
    "\n",
    "class CodeClassifier(nn.Module):\n",
    "    def __init__(self, n_ids=52, n_actions=9,\n",
    "                  in_size=15, code_dim=15, hidden_dim=256, nonlinearity=None):\n",
    "        super(CodeClassifier, self).__init__()\n",
    "        nl = nn.LeakyReLU(0.2) if nonlinearity is None else nonlinearity\n",
    "\n",
    "        self.code_dim = code_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.encoding_fc = nn.Sequential(\n",
    "                nn.Linear(in_size, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim), nl)\n",
    "        \n",
    "        self.id = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "                nn.BatchNorm1d(hidden_dim // 2), nl,\n",
    "                nn.Linear(hidden_dim // 2, n_ids))\n",
    "        self.action = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "                nn.BatchNorm1d(hidden_dim // 2), nl,\n",
    "                nn.Linear(hidden_dim // 2, n_actions))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoding_fc(x)\n",
    "        return self.id(x), self.action(x)\n",
    "       \n",
    "    \n",
    "device = torch.device('cuda:0')\n",
    "model = CodeClassifier(in_size=15)\n",
    "model.to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "\n",
    "mug_train = Codes(id_codes_tr, labels_tr)\n",
    "# sprites_test = Sprites(real_fake_test, labels_both_te)\n",
    "# sprites_train = Sprites(real_imgs_tr, labels_tr)\n",
    "mug_test = Codes(id_codes_te, labels_te)\n",
    "loader = data.DataLoader(mug_train, batch_size=32, shuffle=True, num_workers=4)\n",
    "loader_test = data.DataLoader(mug_test, batch_size=32, shuffle=True, num_workers=4)\n",
    "train_classifier(model, optim, loader, device, 50, './checkpoint_classifier.pth', loader_test) \n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "model = CodeClassifier(in_size=10)\n",
    "model.to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.0004)\n",
    "\n",
    "mug_train = Codes(action_codes_tr, labels_tr)\n",
    "\n",
    "mug_test = Codes(action_codes_te, labels_te)\n",
    "loader = data.DataLoader(mug_train, batch_size=32, shuffle=True, num_workers=4)\n",
    "loader_test = data.DataLoader(mug_test, batch_size=32, shuffle=True, num_workers=4)\n",
    "train_classifier(model, optim, loader, device, 100, './checkpoint_classifier.pth', loader_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
